\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{endfloat}
\renewcommand{\efloatseparator}{\mbox{}} % no new page between figures

\usepackage{booktabs} % For formal tables

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\begin{document}
\title{Bigdata in Clinical Trails}


\author{Mohan Mahendrakar}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{P.O. Box 1212}
  \city{Raleigh} 
  \state{North Carolina} 
  \postcode{43017-6221}
}
\email{mmahendr@iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
The vast volumes of data collected across the clinical trials process offers pharma and biotech 
the opportunity to leverage the information.
ACM SIG Proceedings.
\end{abstract}

\keywords{i523, HID 326, Bigdata, Clinical, Trails, Healthcare, Phase I, Phase II, Phase III, Phase IV}


\maketitle

\section{Introduction}

After transforming customer-facing functions such as sales and marketing, big data is extending its reach to other parts of the enterprise. In research and development, for example, big data and analytics are being adopted across industries, including pharmaceuticals.

\section{Understanding clinical trials}
Clinical trials explore how a treatment reacts in the human body and are designed to
ensure a drug is tolerated and effective before it is licensed by regulatory authorities
and made available for use by doctors. Studies vary in their primary goal or endpoint
(i.e. the most important outcome of the trial), the number of patients involved, and the
specifics of the study design. However, all clinical studies conform to a strict set of
criteria to protect the patients involved and to ensure rigorous evaluation of the drug.

\section{Integrate all data}

Having data that are consistent, reliable, and well linked is one of the biggest challenges facing pharmaceutical clinical Trails. The ability to manage and integrate data generated at all phases of the value chain, from discovery to real-world use after regulatory approval, is a fundamental requirement to allow companies to derive maximum benefit from the technology trends. Data are the foundation upon which the value-adding analytics are built. Effective end-to-end data integration establishes an authoritative source for all pieces of information and accurately links disparate data regardless of the source—be it internal or external, proprietary or publicly available. Data integration also enables comprehensive searches for subsets of data based on the linkages established rather than on the information itself. “Smart” algorithms linking laboratory and clinical data, for example, could create automatic reports that identify related applications or compounds and raise red flags concerning safety or efficacy.

Implementing end-to-end data integration requires a number of capabilities, including trusted sources of data and documents, the ability to establish cross-linkages between elements, robust quality assurance, workflow management, and role-based access to ensure that specific data elements are visible only to those who are authorized to see it. Pharmaceutical companies generally avoid overhauling their entire data-integration system at once because of the logistical challenges and costs involved, although at least one global pharmaceutical enterprise has employed a “big bang” approach to remaking its clinical IT systems.

Data is being generated by different sources and comes in a variety of formats including unstructured
data. All of this data needs to be integrated or ingested into Big Data Repositories or Data Warehouses.
This involves at least three steps, namely, Extract, Transform and Load (ETL). With the ETL processes
that have to be tailored for medical data have to identify and overcome structural, syntactic, and
semantic heterogeneity across the different data sources. The syntactic heterogeneity appears in
forms of different data access interfaces, which were mentioned above, and need to be wrapped and
mediated. Structural heterogeneity refers to different data models and different data schema models
that require integration on schema level. Finally, the process of integration can result in duplication of
data that requires consolidation.

The process of data integration can be further enhanced with information extraction, machine
learning, and semantic web technologies that enable context based information interpretation.
Information extraction will be a mean to obtain data from additional sources for enrichment, which
improves the accuracy of data integration routines, such as duplication and data alignment. Applying
an active learning approach ensures that the deployment of automatic data integration routines will
meet a required level of data quality. Finally, the semantic web technology can be used to generate
graph based knowledge bases and ontologies to represent important concepts and mappings in the
data. The use of standardized ontologies will facilitate collaboration, sharing, modelling, and reuse
across applications. 

\section{Big Data can enhance clinical research by}
discovering hidden patterns and associations within the heterogeneous data, uncovering new
biomarkers and drug targets. Allowing the development of predictive disease progression models. Analyzing Real World Data (RWD) as a complementary instrument to clinical trials, for the rapid development of new personalized medicines. The development of advanced statistical methods for learning causal relations from large scale observational data is a crucial element
for this analysis

\section{Exascale computing}
There will be use cases, e.g. precision medicine, where the promises brought by Big Data will only be
fulfilled through dramatic improvements in computational performance and capacity, along with
advances in software, tools, and algorithms. Exascale computers—machines that perform one billion
calculations per second and are over 100 times more powerful than today’s fastest systems—will be
needed to analyse vast stores of clinical and genomic data and develop predictive treatments based
on advanced 3D multi-scale simulations with uncertainty quantification. Precision medicine will also
require scaling these systems down, so clinicians can incorporate research breakthroughs into
everyday practice. 

\section{Oncology is undergoing a data-driven metamorphosis}
Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as "big data", is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.

\section{Big data analytics}
Medical research has always been a data-driven science, with randomized clinical trials being a gold
standard in many cases. However, due to recent advances in omics-technologies, medical imaging,
comprehensive electronic health records, and smart devices, medical research as well as clinical
practice are quickly changing into Big Data-driven fields. As such, the healthcare domain as a whole -
doctors, patients, management, insurance, and politics - can significantly profit from current advances
in Big Data technologies, and in particular from analytics. 

\section{Advanced Machine Learning and Reinforcement Learning 
}
Many healthcare applications would significantly benefit from the processing and analysis of multimodal
data – such as images, signals, video, 3D models, genomic sequences, reports, etc. Advanced
machine learning systems can be used to learn and relate information from multiple sources and
identify hidden correlations not visible when considering only one source of data. For instance,
combining features from images (e.g. CT scans, radiographs) and text (e.g. clinical reports) can
significantly improve the performance of solutions. 


\section{Challenges}
Big pharma companies typically keep their cards close to the vest because it costs so much to develop a drug throughout its lifetime.  From discovery to prescription pad, a typical medication can take twelve years and \$4 billion to shepherd through its lifecycle, a significant investment that would be hard to recoup if everyone had the secret to the newest blockbuster pill, especially since only ten percent of drugs ever make it to market.

Although there is already a huge amount of healthcare data around the world and while it is growing
at an exponential rate, nearly all of the data is stored in individually. Data collected by a  clinic
or by a hospital is mostly kept within the boundaries of the healthcare provider. Moreover, data stored
within a hospital is hardly ever integrated across multiple IT systems. For example, if we consider all
the available data at a hospital from a single patient’s perspective, information about the patient will
exist in the EMR system, laboratory, imaging system and prescription databases. Information
describing which doctors and nurses attended to the specific patient will also exist. However, in the
vast majority of cases, every data source mentioned here is stored in separate silos. Thus deriving
insights and therefore value from the aggregation of these data sets is not possible at this stage. It is
also important to realize that in today’s world a patient’s medical data does not only reside within the
boundaries of a healthcare provider. The medical insurance and pharmaceuticals industries also hold
information about specific claims and the characteristics of prescribed drugs respectively. Increasingly,
patient-generated data from IoT devices such as fitness trackers, blood pressure monitors and
weighing scales are also providing critical information about the day-to-day lifestyle characteristics of
an individual. Insights derived from such data generated by the linking among EMR data, vital data,
laboratory data, medication information, symptoms (to mention some of these) and their aggregation,
even more with doctor notes, patient discharge letters, patient diaries, medical publications, namely
linking structured with unstructured data, can be crucial to design coaching programs that would
help improve peoples’ lifestyles and eventually reduce incidences of chronic disease, medication and
hospitalization. 

\begin{acks}

  The authors would like to thank to professor and TAs for guiding.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
